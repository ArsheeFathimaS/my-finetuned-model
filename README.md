# ðŸ¤– my-finetuned-model

A fine-tuned **OpenChat 3.5** model customized using a domain-specific **JSONL dataset** and deployed with an **ASR â†’ LLM â†’ TTS** conversational pipeline using **Gradio**.

---

## ðŸŒ Live Demo on Hugging Face

[![Hugging Face](https://img.shields.io/badge/HuggingFace-Model-yellow?logo=huggingface)](https://huggingface.co/arsheefathimas/my-finetuned-openchat)  
ðŸ‘‰ [Click here to view and download the model](https://huggingface.co/arsheefathimas/my-finetuned-openchat)

---

## ðŸš€ Key Highlights

- ðŸ§  Fine-tuned version of OpenChat 3.5  
- ðŸ—£ï¸ Converts voice to text using **Whisper Tiny**  
- ðŸ’¬ Generates context-aware replies using the fine-tuned model  
- ðŸ”Š Responds with voice using **gTTS (Google Text-to-Speech)**  
- ðŸ§© Built with Gradio and runs inside Colab (A100 GPU used)

---

## ðŸ§ª Base Model

- **Base:** [OpenChat 3.5](https://huggingface.co/openchat/openchat_3.5)  
- **Architecture:** Mistral  
- **Params:** 7.24B  
- **Precision:** FP16  
- **Chat Format:** ChatML / OpenChat-style templating

---

## ðŸ› ï¸ Training Setup

- **Data format:** JSONL with role-based dialogue  
- **Fine-tuning:** Used LLaMA Factory + Flash Attention 2 on Colab A100  
- **Epochs:** 3  
- **Batch Size:** 1  
- **Tokenizer:** Inherited from base OpenChat model

---

## ðŸ”— Repositories

- ðŸ’¾ **Model:** [Hugging Face - arsheefathimas/my-finetuned-openchat](https://huggingface.co/arsheefathimas/my-finetuned-openchat)  
- ðŸ’» **Gradio App:** [Hugging Face Space - Yalnaverse Chat](https://huggingface.co/spaces/arsheefathimas/yalnaverse_chat)  
- ðŸ“¦ **Source Code (this repo):** [`my-finetuned-model`](https://github.com/ArsheeFathimaS/my-finetuned-model)

---

## ðŸªª License

> ðŸ“„ Apache 2.0 â€” Same as the base OpenChat model.

---

## ðŸ’¡ How to Use

```python
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

tokenizer = AutoTokenizer.from_pretrained("arsheefathimas/my-finetuned-openchat")
model = AutoModelForCausalLM.from_pretrained("arsheefathimas/my-finetuned-openchat")

generator = pipeline("text-generation", model=model, tokenizer=tokenizer)
prompt = "I'm unable to pass level 21. Can you help?"
print(generator(prompt, max_new_tokens=100)[0]["generated_text"])

